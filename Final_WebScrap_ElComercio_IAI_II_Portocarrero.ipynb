{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VNzNkWAYNBIV"
      },
      "outputs": [],
      "source": [
        "#Importacion de Librerias\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Montamos la unidad de Drive al Workspace\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zby-lCu6NLUI",
        "outputId": "79f6bc63-db28-4870-e88f-28b9bb9bc981"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Estableciendo fecha de inicio y fin para el WC de noticias\n",
        "url_base = 'https://elcomercio.pe/archivo/todas/'\n",
        "fecha_inicio = '2023-08-25'\n",
        "fecha_fin = '2023-10-05'"
      ],
      "metadata": {
        "id": "O9d8E7iaNf9c"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Almacenamiento de fechas\n",
        "dtdf = []"
      ],
      "metadata": {
        "id": "wGutzU8uN3RG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bucle para la recoleccion de noticias\n",
        "current_fecha = fecha_inicio\n",
        "while current_fecha <= fecha_fin:\n",
        "    # URL Formateada (Fecha Actual)\n",
        "    url = f'{url_base}{current_fecha}/'\n",
        "\n",
        "    # Usamos REQUESTS para la URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Validacion del estado de respuesta\n",
        "    if response.status_code == 200:\n",
        "        content = response.text\n",
        "\n",
        "        # Parsing HTML con BS4\n",
        "        soup = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "        # Elementos del HTML con informacion relevante\n",
        "        fecha_elements = soup.find_all('span', class_='story-item__date-time')\n",
        "        hora_elements = soup.find_all('span', class_='story-item__date-time')\n",
        "        titulo_elements = soup.find_all('h2', class_='story-item__content-title')\n",
        "        autor_elements = soup.find_all('a', class_='story-item__author block uppercase mt-10 font-thin text-xs text-gray-200')\n",
        "        tema_elements = soup.find_all('a', class_='story-item__section story-item__section--desktop text-sm text-black md:mb-15 hidden')\n",
        "        resumen_elements = soup.find_all('p', class_='story-item__subtitle')\n",
        "\n",
        "        # Extraer el texto de las clases del HTML\n",
        "        fechas = [fecha.text for fecha in fecha_elements[::2]]\n",
        "        horas = [hora.text for hora in hora_elements[1::2]]\n",
        "        titulos = [titulo.find('a').text for titulo in titulo_elements if titulo.find('a')]\n",
        "        autores = [autor.text for autor in autor_elements]\n",
        "        temas = [tema.text for tema in tema_elements]\n",
        "        resumenes = [resumen.text for resumen in resumen_elements]\n",
        "\n",
        "        # Verificar Longitud de listas\n",
        "        min_length = min(len(fechas), len(horas), len(titulos), len(autores))\n",
        "\n",
        "        fechas = fechas[:min_length]\n",
        "        horas = horas[:min_length]\n",
        "        titulos = titulos[:min_length]\n",
        "        autores = autores[:min_length]\n",
        "\n",
        "        # Creacion de un DF\n",
        "        df = pd.DataFrame({'date': fechas, 'time': horas, 'title': titulos, 'author': autores, 'sbuject': temas, 'resume': resumenes})\n",
        "\n",
        "        # Agregar Df's al array\n",
        "        dtdf.append(df)\n",
        "    else:\n",
        "        print(f\"Error: Unable to Acces {current_fecha}.\")\n",
        "\n",
        "    # Avanzar a la siguiente fecha\n",
        "    current_fecha = pd.to_datetime(current_fecha) + pd.DateOffset(days=1)\n",
        "    current_fecha = current_fecha.strftime('%Y-%m-%d')\n",
        "\n",
        "# Concatenar todos los DataFrames en uno solo\n",
        "output = pd.concat(dtdf, ignore_index=True)\n",
        "\n",
        "# Especifica la ruta completa del archivo CSV en tu Google Drive\n",
        "csv_filepath = '/content/drive/MyDrive/InteligenciaArtificial_II/WebScrape_Portocarrero/wc_elcomercio_output_text.csv'\n",
        "\n",
        "# Guardar el DataFrame en un archivo CSV en Google Drive\n",
        "output.to_csv(csv_filepath, index=False, escapechar='\\\\')\n",
        "\n",
        "print(f\"Se ha extraído y guardado el texto en '{csv_filepath}'.\")\n",
        "print(f\"Total de Noticias Adquiridas: \",len(output))"
      ],
      "metadata": {
        "id": "VRMDgwo_N98L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b065095-dd7c-42f5-9e2f-9b1d33d17aee"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Se ha extraído y guardado el texto en '/content/drive/MyDrive/InteligenciaArtificial_II/WebScrape_Portocarrero/wc_elcomercio_output_text.csv'.\n",
            "Total de Noticias Adquiridas:  4200\n"
          ]
        }
      ]
    }
  ]
}